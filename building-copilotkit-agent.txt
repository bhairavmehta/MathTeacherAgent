Introduction
Agent User Interaction Protocol (AG-UI) is a lightweight, event-based protocol that standardizes how AI agents connect to front-end applications. Built for simplicity and flexibility, it enables seamless integration between your AI agents and user interfaces.
AG-UI is complementary to the other two top agentic protocols:

MCP gives agents tools
A2A allows agents to communicate with other agents
AG-UI brings agents into user-facing applications
Zoom image will be displayed

Features
💬 Real-time agentic chat with streaming
🔄 Bi-directional state synchronization
🧩 Generative UI and structured messages
🧠 Real-time context enrichment
🛠️ Frontend tool integration
🧑‍💻 Human-in-the-loop collaboration
CopilotKit is an open-source Agentic Application Framework and hosted service for AI-assisted applications built on top of the Agent User Interaction Protocol (AG-UI).

In this post, we’ll walk through integrating a LangGraph agent deployed with FastAPI into a Next.js application using CopilotKit.

Create LangGraph Agent
First, set up a LangGraph agent to handle the AI logic and deploy the agent using CopilotKit’s FastAPI integration.

Requirements
requires-python = ">=3.12"
dependencies = [
    "copilotkit>=0.1.54",
    "fastapi>=0.115.14",
    "langchain>=0.3.26",
    "langchain-openai>=0.3.27",
    "langgraph>=0.4.10",
    "uvicorn>=0.35.0",
]
Code
Create files agent.py for creating the agent and server.py for deploying the agent.

my-agent
├── agent.py
└── server.py
agent.py
from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages
from langgraph.graph.message import AnyMessage
from langgraph.graph import START, END, StateGraph
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Checkpointer

class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
def chat_node(state: State) -> State:
    model = ChatOpenAI(model_name="gpt-4o", temperature=0, api_key="your openai key")
    state["messages"] = model.invoke(state["messages"])
    return state
    
graph_builder = StateGraph(State)
graph_builder.add_node("chat_node", chat_node)
graph_builder.add_edge(START, "chat_node")
graph_builder.add_edge("chat_node", END)
graph = graph_builder.compile(checkpointer=MemorySaver())
2. server.py

from copilotkit.integrations.fastapi import add_fastapi_endpoint
from copilotkit import CopilotKitRemoteEndpoint, LangGraphAgent
from agent import graph
from fastapi import FastAPI

app = FastAPI()

# Initialize the CopilotKit SDK 
sdk = CopilotKitRemoteEndpoint(agents=[
        LangGraphAgent(
            name="agent",
            description="An example agent to use as a starting point for your own agent.",
            graph=graph,
        )
    ], actions=[])

# Add the CopilotKit endpoint to your FastAPI app 
add_fastapi_endpoint(app, sdk, "/copilotkit_remote", max_workers=10)

def main():
    """Run the uvicorn server."""
    import uvicorn
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == "__main__":
    main()
Execute the server

python server.py
Check the CopilotKit endpoint: http://localhost:8000/copilotkit_remote/

Output
Zoom image will be displayed

Create the Next.js App
Create a new Next.js application.

npx create-next-app@latest
Install CopilotKit: Install the necessary CopilotKit packages for the frontend.

npm install @copilotkit/react-ui @copilotkit/react-core
Install Copilot Runtime: Copilot Runtime is a production-ready proxy for your LangGraph agents.

npm install @copilotkit/runtime class-validator
Set Up a Copilot Runtime Endpoint
Create a new route to handle the /api/copilotkit endpoint in your Next.js app (app/api/copilotkit/route.ts).

my-app
└── app
    ├── api
    │   └── copilotkit
    │       └── route.ts
    ├── layout.tsx
    └── page.tsx
import {
  CopilotRuntime,
  ExperimentalEmptyAdapter,
  copilotRuntimeNextJSAppRouterEndpoint,
} from "@copilotkit/runtime";
import { NextRequest } from "next/server";

// You can use any service adapter here for multi-agent support.
const serviceAdapter = new ExperimentalEmptyAdapter();
const runtime = new CopilotRuntime({
  remoteEndpoints: [
    // Our FastAPI endpoint URL
    { url: "http://localhost:8000/copilotkit_remote" },
  ],
});

export const POST = async (req: NextRequest) => {
  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({
    runtime,
    serviceAdapter,
    endpoint: "/api/copilotkit",
  });
  return handleRequest(req);
};
Configure the CopilotKit Provider
Wrap your application with the <CopilotKit> component in layout.tsx to enable CopilotKit functionality.

The <CopilotKit> component must wrap the Copilot-aware parts of your application. For most use-cases, it's appropriate to wrap the CopilotKit provider around the entire app

import "./globals.css";
import { ReactNode } from "react";
import { CopilotKit } from "@copilotkit/react-core";

export default function RootLayout({ children }: { children: ReactNode }) {
  return (
    <html lang="en">
      <body>
        <CopilotKit
          runtimeUrl="/api/copilotkit"
          agent="agent" // the name of the agent you want to use
        >
          {children}
        </CopilotKit>
      </body>
    </html>
  );
}
Choose a Copilot UI
Import the default CopilotKit styles in your root component (layout.tsx).

import "@copilotkit/react-ui/styles.css";
Available UIs

CopilotChat
CopilotSidebar
CopilotPopup
Headless UI
For this example, we’ll use CopilotSidebar, which provides a collapsible and expandable sidebar chat interface.

import { CopilotSidebar } from "@copilotkit/react-ui";

export function YourApp() {
  return (
    <CopilotSidebar
      defaultOpen={true}
      instructions={"You are assisting the user as best as you can. Answer in the best way possible given the data you have."}
      labels={{
        title: "Sidebar Assistant",
        initial: "How can I help you today?",
      }}
    >
      <YourMainContent />
    </CopilotSidebar>
  );
}
Updated layout.tsx
Here’s the complete layout.tsx with the CopilotSidebar integrated.

my-app
└── app
    ├── api
    │   └── copilotkit
    │       └── route.ts
    ├── layout.tsx
    └── page.tsx
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";
import { CopilotKit } from "@copilotkit/react-core";
import "@copilotkit/react-ui/styles.css";
import { CopilotSidebar } from "@copilotkit/react-ui";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        <CopilotKit
          runtimeUrl="/api/copilotkit"
          agent="agent" // the name of the agent you want to use
        >
          <CopilotSidebar
            defaultOpen={true}
            instructions={"You are assisting the user as best as you can. Answer in the best way possible given the data you have."}
            labels={{
              title: "Sidebar Assistant",
              initial: "How can I help you today?",
            }}
          >
            {children}
          </CopilotSidebar>
        </CopilotKit>
      </body>
    </html>
  );
}
Execute the Next.js app

npm run dev
Check the next.js app: http://localhost:3000

Output
Zoom image will be displayed

Talk to Your Agent!
Congratulations! You’ve successfully integrated a LangGraph agent chatbot into your Next.js application using CopilotKit. Start by asking your agent a few questions to test the integration.

Adding speech capability (optional)
TTS and STT Endpoints Using ElevenLabs
Add text-to-speech (TTS) and speech-to-text (STT) endpoints to your FastAPI server using ElevenLabs Python SDK for enhanced interaction capabilities.

Add the following code to server.py

from elevenlabs.client import ElevenLabs

# Load environment variables
load_dotenv()
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

# Initialize ElevenLabs client
elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)

@app.get("/tts")
async def text_to_speech(text: str):
    response = elevenlabs_client.text_to_speech.stream(
        text=text,
        voice_id="IKne3meq5aSn9XLyUdCD",
        model_id="eleven_multilingual_v2",
        output_format="mp3_44100_128",
    )

    audio_stream = BytesIO()

    # Write each chunk of audio data to the stream
    for chunk in response:
        if chunk:
            audio_stream.write(chunk)

    # Reset stream position to the beginning
    audio_stream.seek(0)
    
    return StreamingResponse(
        audio_stream,
        media_type="audio/mpeg",
        headers={"Content-Disposition": "attachment; filename=output.mp3"},
    )

@app.post("/stt")
async def speech_to_text(file: UploadFile = File(...)):
    # Read audio data
    audio_data = await file.read()
    if not audio_data:
        raise HTTPException(status_code=400, detail="No audio data provided.")

    transcription = elevenlabs_client.speech_to_text.convert(
        file=audio_data,
        model_id="scribe_v1", # Model to use, for now only "scribe_v1" is supported
        tag_audio_events=True, # Tag audio events like laughter, applause, etc.
        language_code="eng", # Language of the audio file. If set to None, the model will detect the language automatically.
        diarize=True, # Whether to annotate who is speaking
    )

    return transcription
Adding the Endpoints to CopilotKit
Update the <CopilotKit> component in layout.tsx to include the TTS and STT endpoints.